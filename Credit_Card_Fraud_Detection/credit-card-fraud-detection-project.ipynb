{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objectives","metadata":{}},{"cell_type":"markdown","source":"Perform exploratory  Data Analysis and determine Training Labels\n\n*   Create a column for the class\n*   Standardize the data\n*   Split into training data and test data\n\n\\-Find best Hyperparameter for **Logistic Regression**, **Decision Trees**, **K-Nearest-neighbours (KNN)**, **Support Vector Machine (SVM)**, **Random Forest**, and \n\n*   Find the method performs best using test data\n","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries and Define Auxiliary Functions","metadata":{}},{"cell_type":"markdown","source":"#### We will import the following libraries for this project","metadata":{}},{"cell_type":"code","source":"# Pandas is a software library written for the Python programming language for data manipulation and analysis.\nimport pandas as pd\n# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\nimport numpy as np\n# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\nimport matplotlib.pyplot as plt\n#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\nimport seaborn as sns\n# Preprocessing allows us to standarsize our data\nfrom sklearn import preprocessing\n# Allows us to split our data into training and testing data\nfrom sklearn.model_selection import train_test_split\n# Allows us to test parameters of classification algorithms and find the best one\n#from sklearn.model_selection import GridSearchCV  ## We did not use\n# Allows us to test accuracy by jaccard_score\nfrom sklearn.metrics import jaccard_score\n# Allows us to see the confusion matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n# Allows us to see the classification_report or F1 Score\nfrom sklearn.metrics import classification_report\n# Allows us to see the log loss score\nfrom sklearn.metrics import log_loss\n# Logistic Regression classification algorithm\nfrom sklearn.linear_model import LogisticRegression\n# Support Vector Machine classification algorithm\nfrom sklearn.svm import SVC\nfrom sklearn import svm\n# Decision Tree classification algorithm\nfrom sklearn.tree import DecisionTreeClassifier\n# K Nearest Neighbors classification algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import Library for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:12.081627Z","iopub.execute_input":"2021-11-27T02:36:12.082592Z","iopub.status.idle":"2021-11-27T02:36:13.539373Z","shell.execute_reply.started":"2021-11-27T02:36:12.082526Z","shell.execute_reply":"2021-11-27T02:36:13.538233Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataframe","metadata":{}},{"cell_type":"markdown","source":"#### Load the data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:18.940309Z","iopub.execute_input":"2021-11-27T02:36:18.940615Z","iopub.status.idle":"2021-11-27T02:36:18.950060Z","shell.execute_reply.started":"2021-11-27T02:36:18.940578Z","shell.execute_reply":"2021-11-27T02:36:18.949129Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:22.953951Z","iopub.execute_input":"2021-11-27T02:36:22.954509Z","iopub.status.idle":"2021-11-27T02:36:25.417784Z","shell.execute_reply.started":"2021-11-27T02:36:22.954457Z","shell.execute_reply":"2021-11-27T02:36:25.416736Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Explore the dataset","metadata":{}},{"cell_type":"markdown","source":"#### Shape of the data, head and tail","metadata":{}},{"cell_type":"code","source":"print('1. DATA SHAPE: ', data.shape)\nprint('2. DATA HEAD: '\"\\n\",  data.head(5))\nprint('3. DATA TAIL: '\"\\n\",  data.tail(5))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:34.138112Z","iopub.execute_input":"2021-11-27T02:36:34.139134Z","iopub.status.idle":"2021-11-27T02:36:34.175383Z","shell.execute_reply.started":"2021-11-27T02:36:34.139083Z","shell.execute_reply":"2021-11-27T02:36:34.174266Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Check the types of the data","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:41.082979Z","iopub.execute_input":"2021-11-27T02:36:41.083245Z","iopub.status.idle":"2021-11-27T02:36:41.127137Z","shell.execute_reply.started":"2021-11-27T02:36:41.083217Z","shell.execute_reply":"2021-11-27T02:36:41.126201Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Checking missing values","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:47.998385Z","iopub.execute_input":"2021-11-27T02:36:47.998890Z","iopub.status.idle":"2021-11-27T02:36:48.027435Z","shell.execute_reply.started":"2021-11-27T02:36:47.998838Z","shell.execute_reply":"2021-11-27T02:36:48.026376Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Describe the summary statistics","metadata":{}},{"cell_type":"code","source":"pd.set_option('precision',2)\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:51.711734Z","iopub.execute_input":"2021-11-27T02:36:51.712012Z","iopub.status.idle":"2021-11-27T02:36:52.144143Z","shell.execute_reply.started":"2021-11-27T02:36:51.711982Z","shell.execute_reply":"2021-11-27T02:36:52.143185Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Checking transaction distribution","metadata":{}},{"cell_type":"code","source":"Total_transactions = len(data)\nsecured = len(data[data.Class == 0])\nfraud = len(data[data.Class == 1])\nfraud_percentage = round(fraud/secured*100, 2)\nprint('Total number of Transaction are:', Total_transactions)\nprint('Total number of Secured Transaction are:', secured)\nprint('Total number of Fraud Transaction are:', fraud)\nprint('Percentage of Fraud Transaction are:', fraud_percentage)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:36:55.511231Z","iopub.execute_input":"2021-11-27T02:36:55.511558Z","iopub.status.idle":"2021-11-27T02:36:55.556066Z","shell.execute_reply.started":"2021-11-27T02:36:55.511525Z","shell.execute_reply":"2021-11-27T02:36:55.555187Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Independent Variables","metadata":{}},{"cell_type":"code","source":"X = data.loc[:, data.columns != 'Class']\nX[0:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:13.001828Z","iopub.execute_input":"2021-11-27T02:45:13.002192Z","iopub.status.idle":"2021-11-27T02:45:13.068573Z","shell.execute_reply.started":"2021-11-27T02:45:13.002161Z","shell.execute_reply":"2021-11-27T02:45:13.067940Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### Standardize the data in X ","metadata":{}},{"cell_type":"code","source":"x_scaled = preprocessing.scale(X)\nX = pd.DataFrame(x_scaled)\nX","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:16.238388Z","iopub.execute_input":"2021-11-27T02:45:16.238954Z","iopub.status.idle":"2021-11-27T02:45:16.656695Z","shell.execute_reply.started":"2021-11-27T02:45:16.238914Z","shell.execute_reply":"2021-11-27T02:45:16.655849Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### Dependent Variable","metadata":{}},{"cell_type":"code","source":"y = data[\"Class\"].to_numpy()\ny[0:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:20.661168Z","iopub.execute_input":"2021-11-27T02:45:20.661675Z","iopub.status.idle":"2021-11-27T02:45:20.667328Z","shell.execute_reply.started":"2021-11-27T02:45:20.661624Z","shell.execute_reply":"2021-11-27T02:45:20.666537Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#### Now, we normalize the dataset","metadata":{}},{"cell_type":"code","source":"X = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-27T02:45:23.698047Z","iopub.execute_input":"2021-11-27T02:45:23.698343Z","iopub.status.idle":"2021-11-27T02:45:23.903687Z","shell.execute_reply.started":"2021-11-27T02:45:23.698311Z","shell.execute_reply":"2021-11-27T02:45:23.902847Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Spliting Data into Train and Test Set","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=2)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:28.233901Z","iopub.execute_input":"2021-11-27T02:45:28.234209Z","iopub.status.idle":"2021-11-27T02:45:28.391375Z","shell.execute_reply.started":"2021-11-27T02:45:28.234174Z","shell.execute_reply":"2021-11-27T02:45:28.390485Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression with Scikit-learn","metadata":{}},{"cell_type":"markdown","source":"**C** parameter indicates **inverse of regularization strength** which must be a positive float.","metadata":{}},{"cell_type":"code","source":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:33.237652Z","iopub.execute_input":"2021-11-27T02:45:33.238340Z","iopub.status.idle":"2021-11-27T02:45:35.268544Z","shell.execute_reply.started":"2021-11-27T02:45:33.238291Z","shell.execute_reply":"2021-11-27T02:45:35.267654Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Now we can predict using our test set:","metadata":{}},{"cell_type":"code","source":"yhat = LR.predict(X_test)\nyhat","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:42.357431Z","iopub.execute_input":"2021-11-27T02:45:42.357848Z","iopub.status.idle":"2021-11-27T02:45:42.370958Z","shell.execute_reply.started":"2021-11-27T02:45:42.357818Z","shell.execute_reply":"2021-11-27T02:45:42.369659Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**predict_proba**  returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0, P(Y=0|X), and second column is probability of class 1, P(Y=1|X):","metadata":{}},{"cell_type":"code","source":"yhat_prob = LR.predict_proba(X_test)\nyhat_prob","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:45.873426Z","iopub.execute_input":"2021-11-27T02:45:45.873997Z","iopub.status.idle":"2021-11-27T02:45:45.887105Z","shell.execute_reply.started":"2021-11-27T02:45:45.873961Z","shell.execute_reply":"2021-11-27T02:45:45.886242Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Jaccard Index","metadata":{}},{"cell_type":"markdown","source":"we can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","metadata":{}},{"cell_type":"code","source":"LR_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\n\nLR_AS","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:49.426282Z","iopub.execute_input":"2021-11-27T02:45:49.426607Z","iopub.status.idle":"2021-11-27T02:45:49.446622Z","shell.execute_reply.started":"2021-11-27T02:45:49.426575Z","shell.execute_reply":"2021-11-27T02:45:49.445788Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","metadata":{}},{"cell_type":"code","source":"#yhat=logreg_cv.predict(X_test)\ncm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:45:53.504140Z","iopub.execute_input":"2021-11-27T02:45:53.504418Z","iopub.status.idle":"2021-11-27T02:45:53.779447Z","shell.execute_reply.started":"2021-11-27T02:45:53.504388Z","shell.execute_reply":"2021-11-27T02:45:53.778859Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56870 true positive and 8 false positive result. That explains, out of 56870+8= 56878, we have 56870 successfully classified normal transaction and 8 were falsely classified as normal but they were fraudlent.","metadata":{}},{"cell_type":"markdown","source":"#### Checking F1 Score","metadata":{}},{"cell_type":"code","source":"print (classification_report(y_test, yhat))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:02.127743Z","iopub.execute_input":"2021-11-27T02:46:02.128443Z","iopub.status.idle":"2021-11-27T02:46:02.193467Z","shell.execute_reply.started":"2021-11-27T02:46:02.128400Z","shell.execute_reply":"2021-11-27T02:46:02.192465Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","metadata":{}},{"cell_type":"markdown","source":"### Decision Trees with Scikit-learn","metadata":{}},{"cell_type":"markdown","source":"We will first create an instance of the **DecisionTreeClassifier** called **fraudTree**.\n    Inside of the classifier, specify  *criterion=\"entropy\"* so we can see the information gain of each node.","metadata":{}},{"cell_type":"code","source":"fraudTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nfraudTree # it shows the default parameters","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:07.024883Z","iopub.execute_input":"2021-11-27T02:46:07.025490Z","iopub.status.idle":"2021-11-27T02:46:07.031741Z","shell.execute_reply.started":"2021-11-27T02:46:07.025444Z","shell.execute_reply":"2021-11-27T02:46:07.031120Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Next, we will fit the data with the training feature matrix <b> X_trainset </b> and training  response vector <b> y_trainset </b>","metadata":{}},{"cell_type":"code","source":"fraudTree.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:10.685923Z","iopub.execute_input":"2021-11-27T02:46:10.686940Z","iopub.status.idle":"2021-11-27T02:46:18.671065Z","shell.execute_reply.started":"2021-11-27T02:46:10.686892Z","shell.execute_reply":"2021-11-27T02:46:18.670011Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Let's make some <b>predictions</b> on the testing dataset and store it into a variable called <b>predTree</b>.","metadata":{}},{"cell_type":"code","source":"predTree = fraudTree.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:29.216792Z","iopub.execute_input":"2021-11-27T02:46:29.217119Z","iopub.status.idle":"2021-11-27T02:46:29.226429Z","shell.execute_reply.started":"2021-11-27T02:46:29.217085Z","shell.execute_reply":"2021-11-27T02:46:29.225605Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"You can print out <b>predTree</b> and <b>y_test</b> if you want to visually compare the predictions to the actual values.","metadata":{}},{"cell_type":"code","source":"print (predTree [0:5])\nprint (y_test [0:5])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:35.872984Z","iopub.execute_input":"2021-11-27T02:46:35.874079Z","iopub.status.idle":"2021-11-27T02:46:35.880093Z","shell.execute_reply.started":"2021-11-27T02:46:35.874035Z","shell.execute_reply":"2021-11-27T02:46:35.879143Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Jaccard Index","metadata":{}},{"cell_type":"markdown","source":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","metadata":{}},{"cell_type":"code","source":"DT_AS = round(jaccard_score(y_test, predTree,pos_label=0)*100,2)\n\nDT_AS","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:41.381790Z","iopub.execute_input":"2021-11-27T02:46:41.382957Z","iopub.status.idle":"2021-11-27T02:46:41.405611Z","shell.execute_reply.started":"2021-11-27T02:46:41.382896Z","shell.execute_reply":"2021-11-27T02:46:41.405032Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, predTree, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:45.768888Z","iopub.execute_input":"2021-11-27T02:46:45.769408Z","iopub.status.idle":"2021-11-27T02:46:46.016574Z","shell.execute_reply.started":"2021-11-27T02:46:45.769369Z","shell.execute_reply":"2021-11-27T02:46:46.015646Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56867 true positive and 11 false positive result. That explains, out of 56867+11= 56878, we have 56867 successfully classified normal transaction and 11 were falsely classified as normal but they were fraudlent.","metadata":{}},{"cell_type":"markdown","source":"#### Checking F1 Score","metadata":{}},{"cell_type":"code","source":"print (classification_report(y_test, predTree))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:50.306870Z","iopub.execute_input":"2021-11-27T02:46:50.307218Z","iopub.status.idle":"2021-11-27T02:46:50.375623Z","shell.execute_reply.started":"2021-11-27T02:46:50.307181Z","shell.execute_reply":"2021-11-27T02:46:50.374530Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","metadata":{}},{"cell_type":"markdown","source":"### K-Nearest-neighbours with Scikit-learn","metadata":{}},{"cell_type":"markdown","source":"Let's start the training algorithm with k=4 for now:","metadata":{}},{"cell_type":"code","source":"k = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:55.013933Z","iopub.execute_input":"2021-11-27T02:46:55.014206Z","iopub.status.idle":"2021-11-27T02:46:56.843988Z","shell.execute_reply.started":"2021-11-27T02:46:55.014177Z","shell.execute_reply":"2021-11-27T02:46:56.843117Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"We can use the model to make predictions on the test set:","metadata":{}},{"cell_type":"code","source":"yhat = neigh.predict(X_test)\nyhat[0:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:46:58.006011Z","iopub.execute_input":"2021-11-27T02:46:58.006685Z","iopub.status.idle":"2021-11-27T02:56:07.327364Z","shell.execute_reply.started":"2021-11-27T02:46:58.006642Z","shell.execute_reply":"2021-11-27T02:56:07.326313Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Jaccard Index","metadata":{}},{"cell_type":"markdown","source":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","metadata":{}},{"cell_type":"code","source":"KNN_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\n\nKNN_AS","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:56:07.329217Z","iopub.execute_input":"2021-11-27T02:56:07.329493Z","iopub.status.idle":"2021-11-27T02:56:07.350920Z","shell.execute_reply.started":"2021-11-27T02:56:07.329462Z","shell.execute_reply":"2021-11-27T02:56:07.350330Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T02:56:07.351846Z","iopub.execute_input":"2021-11-27T02:56:07.352620Z","iopub.status.idle":"2021-11-27T02:56:07.603030Z","shell.execute_reply.started":"2021-11-27T02:56:07.352570Z","shell.execute_reply":"2021-11-27T02:56:07.602211Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56875 true positive and 3 false positive result. That explains, out of 56875+3= 56878, we have 56875 successfully classified normal transaction and 3 were falsely classified as normal but they were fraudlent.","metadata":{}},{"cell_type":"markdown","source":"#### Checking F1 Score","metadata":{}},{"cell_type":"code","source":"print (classification_report(y_test, yhat))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:09:30.570068Z","iopub.execute_input":"2021-11-27T03:09:30.570385Z","iopub.status.idle":"2021-11-27T03:09:30.635517Z","shell.execute_reply.started":"2021-11-27T03:09:30.570344Z","shell.execute_reply":"2021-11-27T03:09:30.634403Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","metadata":{}},{"cell_type":"markdown","source":"### Support Vector Machine with Scikit-learn","metadata":{}},{"cell_type":"markdown","source":"Let's start with Radial Basis Function (RBF) kernel.","metadata":{}},{"cell_type":"code","source":"clf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) ","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:09:36.721372Z","iopub.execute_input":"2021-11-27T03:09:36.722202Z","iopub.status.idle":"2021-11-27T03:22:29.448416Z","shell.execute_reply.started":"2021-11-27T03:09:36.722152Z","shell.execute_reply":"2021-11-27T03:22:29.447453Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"After being fitted, the model can then be used to predict new values:","metadata":{}},{"cell_type":"code","source":"yhat = clf.predict(X_test)\nyhat [0:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:22:29.450550Z","iopub.execute_input":"2021-11-27T03:22:29.450887Z","iopub.status.idle":"2021-11-27T03:22:41.155156Z","shell.execute_reply.started":"2021-11-27T03:22:29.450844Z","shell.execute_reply":"2021-11-27T03:22:41.154320Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Jaccard Index","metadata":{}},{"cell_type":"markdown","source":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","metadata":{}},{"cell_type":"code","source":"SVM_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\n\nSVM_AS","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:22:41.156575Z","iopub.execute_input":"2021-11-27T03:22:41.157231Z","iopub.status.idle":"2021-11-27T03:22:41.178111Z","shell.execute_reply.started":"2021-11-27T03:22:41.157198Z","shell.execute_reply":"2021-11-27T03:22:41.177205Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:09:27.556909Z","iopub.execute_input":"2021-11-27T03:09:27.557313Z","iopub.status.idle":"2021-11-27T03:09:27.803071Z","shell.execute_reply.started":"2021-11-27T03:09:27.557269Z","shell.execute_reply":"2021-11-27T03:09:27.802134Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56874 true positive and 4 false positive result. That explains, out of 56874+4= 56878, we have 56874 successfully classified normal transaction and 4 were falsely classified as normal but they were fraudlent.","metadata":{}},{"cell_type":"markdown","source":"#### Checking F1 Score","metadata":{}},{"cell_type":"code","source":"print (classification_report(y_test, yhat))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:22:41.179736Z","iopub.execute_input":"2021-11-27T03:22:41.179956Z","iopub.status.idle":"2021-11-27T03:22:41.246430Z","shell.execute_reply.started":"2021-11-27T03:22:41.179930Z","shell.execute_reply":"2021-11-27T03:22:41.245721Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest with Scikit-learn","metadata":{}},{"cell_type":"markdown","source":"Initialize the Random Forest","metadata":{}},{"cell_type":"code","source":"RF = RandomForestClassifier()\n\n#Train the model using Training Dataset\nRF.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:29:08.474206Z","iopub.execute_input":"2021-11-27T03:29:08.474568Z","iopub.status.idle":"2021-11-27T03:33:45.921451Z","shell.execute_reply.started":"2021-11-27T03:29:08.474532Z","shell.execute_reply":"2021-11-27T03:33:45.920509Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"After being fitted, the model can then be used to predict new values:","metadata":{}},{"cell_type":"code","source":"yhat = RF.predict(X_test)\nyhat [0:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:33:45.922946Z","iopub.execute_input":"2021-11-27T03:33:45.923222Z","iopub.status.idle":"2021-11-27T03:33:46.541795Z","shell.execute_reply.started":"2021-11-27T03:33:45.923192Z","shell.execute_reply":"2021-11-27T03:33:46.541106Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Jaccard Index","metadata":{}},{"cell_type":"markdown","source":"We can define jaccard as the size of the intersection divided by the size of the union of the two label sets. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","metadata":{}},{"cell_type":"code","source":"RF_AS = round(jaccard_score(y_test, yhat,pos_label=0)*100,2)\nRF_AS\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:33:46.542863Z","iopub.execute_input":"2021-11-27T03:33:46.543241Z","iopub.status.idle":"2021-11-27T03:33:46.564156Z","shell.execute_reply.started":"2021-11-27T03:33:46.543210Z","shell.execute_reply":"2021-11-27T03:33:46.563281Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy Evaluation by Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yhat, labels=[1,0])\nprint('Confusion matrix:''\\n', cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:33:46.565780Z","iopub.execute_input":"2021-11-27T03:33:46.566084Z","iopub.status.idle":"2021-11-27T03:33:46.810840Z","shell.execute_reply.started":"2021-11-27T03:33:46.566052Z","shell.execute_reply":"2021-11-27T03:33:46.809914Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"In this confusion matrix, the first row presents the negetive and second row presents the positive result. So we have a total of 56873 true positive and 5 false positive result. That explains, out of 56873+5= 56878, we have 56873 successfully classified normal transaction and 5 were falsely classified as normal but they were fraudlent.","metadata":{}},{"cell_type":"markdown","source":"#### Checking F1 Score","metadata":{}},{"cell_type":"code","source":"print (classification_report(y_test, yhat))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:33:46.811986Z","iopub.execute_input":"2021-11-27T03:33:46.812357Z","iopub.status.idle":"2021-11-27T03:33:46.878175Z","shell.execute_reply.started":"2021-11-27T03:33:46.812310Z","shell.execute_reply":"2021-11-27T03:33:46.877540Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Based on the count of each section, we can calculate precision and recall of each label:\n\n*   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n\n*   **Recall** is the true positive rate. It is defined as: Recall =  TP / (TP + FN)\n\nSo, we can calculate the precision and recall of each class.\n\n**F1 score:**\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label.\n\nThe F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\nFinally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 1.0 in our case.","metadata":{}},{"cell_type":"markdown","source":"## Best Performed Method","metadata":{}},{"cell_type":"code","source":"BPM = pd.DataFrame({\n    'Models': ['Logistic Regression', 'Decision Trees','K-Nearest-neighbours (KNN)', \n               'Support Vector Machine (SVM)', 'Random Forest'],\n    'Accuracy_Score': [LR_AS, DT_AS, KNN_AS, SVM_AS, RF_AS]})\n\nBPM.sort_values(by='Accuracy_Score', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:34:28.860026Z","iopub.execute_input":"2021-11-27T03:34:28.860496Z","iopub.status.idle":"2021-11-27T03:34:28.873834Z","shell.execute_reply.started":"2021-11-27T03:34:28.860463Z","shell.execute_reply":"2021-11-27T03:34:28.873204Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"*   We found **0.17%** Fraud Transaction were occured.\n*   We found **99.96%** accuracy score for **Random Forest**. \n ","metadata":{}},{"cell_type":"markdown","source":"# Thank you very much for reading this NoteBook. Please share your comments and suggestions for improving more.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}